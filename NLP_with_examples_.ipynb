{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "üß† **Introduction to NLP (Natural Language Processing)**\n",
        "\n",
        "Natural Language Processing (NLP) is a branch of AI that allows machines to understand, interpret, and respond to human language. NLP bridges the gap between human communication and computer understanding.\n",
        "\n",
        "üîç **Common Applications of NLP:**\n",
        "\n",
        "1. Chatbots and Virtual Assistants (like ChatGPT, Siri)\n",
        "\n",
        "2. Sentiment Analysis (e.g., is a tweet positive or negative?)\n",
        "\n",
        "3. Machine Translation (e.g., Google Translate)\n",
        "\n",
        "4. Text Summarization\n",
        "\n",
        "5. Search Engines (Google, Bing)\n",
        "\n",
        "6. Spam Detection"
      ],
      "metadata": {
        "id": "KtAYp4UWa-Ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElpYxh3356cQ",
        "outputId": "54101c09-10a0-4790-942f-065deb5f1a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = (\n",
        "    '''\n",
        "    The children are playing in the garden. She enjoys reading books on the weekend. They were walking slowly along the river. He studies hard to achieve his goals.\n",
        "    '''\n",
        ")"
      ],
      "metadata": {
        "id": "wP9pRngncgOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25NdaIQ0dhiq",
        "outputId": "1280333f-27f4-4f19-d047-6c6947015239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    The children are playing in the garden. She enjoys reading books on the weekend. They were walking slowly along the river. He studies hard to achieve his goals.\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When working with text (called a corpus), we follow a few key preprocessing steps before feeding it into an NLP model.\n",
        "\n",
        "1. **Tokenization**\n",
        "\n",
        "This is the first and most important step in NLP. It splits text into smaller units.\n",
        "\n",
        "### üì¶ Types of Tokenization\n",
        "\n",
        "| **Tokenizer**              | **Description**                                                 |\n",
        "|---------------------------|-----------------------------------------------------------------|\n",
        "| **Word Tokenizer**         | Splits text into individual words                              |\n",
        "| **Sentence Tokenizer**     | Splits text into complete sentences                            |\n",
        "| **WordPunct Tokenizer**    | Splits words and punctuation separately                        |\n",
        "| **Treebank Word Tokenizer**| Handles contractions and punctuation more accurately           |\n"
      ],
      "metadata": {
        "id": "IiC4nRh0dIaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPL7jiSoeu7p",
        "outputId": "a8f74235-631f-413d-f79f-d00b2fb4fad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi8CRlpudk5C",
        "outputId": "3c9e08ae-bb43-4e38-dfd2-4c39e4f3679f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'children', 'are', 'playing', 'in', 'the', 'garden', '.', 'She', 'enjoys', 'reading', 'books', 'on', 'the', 'weekend', '.', 'They', 'were', 'walking', 'slowly', 'along', 'the', 'river', '.', 'He', 'studies', 'hard', 'to', 'achieve', 'his', 'goals', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "print(sent_tokenize(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaePRgyidk2o",
        "outputId": "934e7174-a6fc-4c56-aa30-51d0eabc7a83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n    The children are playing in the garden.', 'She enjoys reading books on the weekend.', 'They were walking slowly along the river.', 'He studies hard to achieve his goals.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "print(wordpunct_tokenize(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAtGE2htdkrs",
        "outputId": "83803dc4-656a-4f04-a8db-241665a1d325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'children', 'are', 'playing', 'in', 'the', 'garden', '.', 'She', 'enjoys', 'reading', 'books', 'on', 'the', 'weekend', '.', 'They', 'were', 'walking', 'slowly', 'along', 'the', 'river', '.', 'He', 'studies', 'hard', 'to', 'achieve', 'his', 'goals', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "print(tokenizer.tokenize(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTJJQ_xLe6mg",
        "outputId": "66c3738f-1650-4b82-930c-a74fe9d2c58a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'children', 'are', 'playing', 'in', 'the', 'garden.', 'She', 'enjoys', 'reading', 'books', 'on', 'the', 'weekend.', 'They', 'were', 'walking', 'slowly', 'along', 'the', 'river.', 'He', 'studies', 'hard', 'to', 'achieve', 'his', 'goals', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Stemmer**\n",
        "\n",
        "A Stemmer is a tool in NLP that reduces a word to its base or root form ‚Äî called the stem. The stem may not always be a valid word, but it helps group similar words together for analysis.\n",
        "\n",
        "Types of stemmer:\n",
        "\n",
        "| **Stemmer**         | **Description**                                                  |\n",
        "|---------------------|------------------------------------------------------------------|\n",
        "| Porter Stemmer      | Rule-based, widely used, moderate stemming                       |\n",
        "| Lancaster Stemmer   | Very aggressive, can chop off too much                           |\n",
        "| Snowball Stemmer    | Refined Porter, supports multiple languages                      |\n",
        "| Regexp Stemmer      | Uses regular expressions to remove suffixes (customizable)       |\n"
      ],
      "metadata": {
        "id": "Ku6aAqdWfGZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "sentence = \"The runners were running quickly towards the finishing line.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "porter = PorterStemmer()\n",
        "print(\"PorterStemmer:\", [porter.stem(word) for word in tokens])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWtWmfnDf2Mb",
        "outputId": "6ba1f1e9-44ce-47db-f6e2-6339078fd588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'runners', 'were', 'running', 'quickly', 'towards', 'the', 'finishing', 'line', '.']\n",
            "PorterStemmer: ['the', 'runner', 'were', 'run', 'quickli', 'toward', 'the', 'finish', 'line', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "sentence = \"The runners were running quickly towards the finishing line.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "lancaster = LancasterStemmer()\n",
        "print(\"LancasterStemmer:\", [lancaster.stem(word) for word in tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RNA6ScXhaSw",
        "outputId": "d1fbfcaa-4555-44f4-84bb-f74860e288d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'runners', 'were', 'running', 'quickly', 'towards', 'the', 'finishing', 'line', '.']\n",
            "LancasterStemmer: ['the', 'run', 'wer', 'run', 'quick', 'toward', 'the', 'fin', 'lin', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "sentence = \"The runners were running quickly towards the finishing line.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "print(\"SnowballStemmer:\", [snowball.stem(word) for word in tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lptPGdgChmi5",
        "outputId": "2f063098-52b5-46db-810e-47044bbe2855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'runners', 'were', 'running', 'quickly', 'towards', 'the', 'finishing', 'line', '.']\n",
            "SnowballStemmer: ['the', 'runner', 'were', 'run', 'quick', 'toward', 'the', 'finish', 'line', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import RegexpStemmer\n",
        "\n",
        "sentence = \"The runners were running quickly towards the finishing line.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "regex = RegexpStemmer('ing$|s$|e$|ners$', min=4)\n",
        "print(\"RegexpStemmer:\", [regex.stem(word) for word in tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i1J4lV5hv4f",
        "outputId": "f741cc61-dc2f-4a02-8d4b-328b134f2781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'runners', 'were', 'running', 'quickly', 'towards', 'the', 'finishing', 'line', '.']\n",
            "RegexpStemmer: ['The', 'run', 'wer', 'runn', 'quickly', 'toward', 'the', 'finish', 'lin', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Lemmatization**\n",
        "\n",
        "Lemmatization is the process of reducing a word to its dictionary root form (lemma). Unlike stemming, lemmatization gives real words and considers the context and part of speech (POS).\n",
        "\n",
        "\n",
        "### üå± Types of Lemmatizers\n",
        "\n",
        "| **Lemmatizer**        | **Library/Tool** | **Description**                                                        |\n",
        "|-----------------------|------------------|------------------------------------------------------------------------|\n",
        "| **WordNetLemmatizer** | NLTK             | Rule-based lemmatizer using WordNet; supports POS tagging              |\n",
        "| **spaCy Lemmatizer**  | spaCy            | Context-aware lemmatizer using statistical models                      |\n",
        "| **TextBlob Lemmatizer**| TextBlob        | Simple wrapper around WordNet; easy to use for beginners               |\n",
        "                    |\n",
        "\n"
      ],
      "metadata": {
        "id": "CWlmnqzxitJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Downloads\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Sample sentence\n",
        "sentence = \"The runners were running quickly towards the finishing line.\"\n",
        "\n",
        "# Tokenize sentence\n",
        "tokens = word_tokenize(sentence)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Basic lemmatization (default POS = noun)\n",
        "print(\"Lemmatized (default noun):\", [lemmatizer.lemmatize(word) for word in tokens])\n",
        "\n",
        "print(\"Lemmatized (verb):\", [lemmatizer.lemmatize(word,pos='v') for word in tokens])\n",
        "\n",
        "print(\"Lemmatized (adjective):\", [lemmatizer.lemmatize(word,pos='a') for word in tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCO9775GjzSJ",
        "outputId": "ccce4e87-0f75-4e96-f269-fe69936e2a21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'runners', 'were', 'running', 'quickly', 'towards', 'the', 'finishing', 'line', '.']\n",
            "Lemmatized (default noun): ['The', 'runner', 'were', 'running', 'quickly', 'towards', 'the', 'finishing', 'line', '.']\n",
            "Lemmatized (verb): ['The', 'runners', 'be', 'run', 'quickly', 'towards', 'the', 'finish', 'line', '.']\n",
            "Lemmatized (adjective): ['The', 'runners', 'were', 'running', 'quickly', 'towards', 'the', 'finishing', 'line', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Install and load spaCy\n",
        "!pip install -q spacy\n",
        "import spacy\n",
        "\n",
        "# Load English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample sentence\n",
        "sentence = \"The runners were running quickly towards the finishing line.\"\n",
        "\n",
        "# Process sentence with spaCy\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# All tokens and their lemmas\n",
        "print(\" Tokens:\", [token.text for token in doc])\n",
        "print(\"Lemmatized (auto POS):\", [token.lemma_ for token in doc])\n",
        "\n",
        "# Filter: Lemmas of words that are verbs\n",
        "verbs = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
        "print(\" Lemmatized (verbs only):\", verbs)\n",
        "\n",
        "# Filter: Lemmas of words that are adjectives\n",
        "adjectives = [token.lemma_ for token in doc if token.pos_ == \"ADJ\"]\n",
        "print(\" Lemmatized (adjectives only):\", adjectives)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVHtCCKVlvdv",
        "outputId": "b2acfd05-1756-467a-9586-22f2ad1d9643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Tokens: ['The', 'runners', 'were', 'running', 'quickly', 'towards', 'the', 'finishing', 'line', '.']\n",
            "Lemmatized (auto POS): ['the', 'runner', 'be', 'run', 'quickly', 'towards', 'the', 'finish', 'line', '.']\n",
            " Lemmatized (verbs only): ['run', 'finish']\n",
            " Lemmatized (adjectives only): []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Install TextBlob and download corpora\n",
        "!pip install -q textblob\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "!python -m textblob.download_corpora\n",
        "# Sample sentence\n",
        "sentence = \"The runners were running quickly towards the finishing line.\"\n",
        "\n",
        "# Create TextBlob object\n",
        "blob = TextBlob(sentence)\n",
        "\n",
        "# Tokenize and lemmatize\n",
        "print(\" Tokens:\", blob.words)\n",
        "\n",
        "# Lemmatized words (TextBlob assumes default noun)\n",
        "lemmatized_default = [word.lemmatize() for word in blob.words]\n",
        "print(\" Lemmatized (default):\", lemmatized_default)\n",
        "\n",
        "# Simulated POS lemmatization using TextBlob tags\n",
        "print(\" Tagged words:\", blob.tags)\n",
        "\n",
        "# Apply lemmatization with simple POS mapping\n",
        "lemmatized_with_pos = [word.lemmatize(pos[0].lower()) if pos[0].lower() in ['a','n','v'] else word.lemmatize() for word, pos in blob.tags]\n",
        "print(\" Lemmatized (with POS):\", lemmatized_with_pos)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZJLRUoBmFnA",
        "outputId": "71c9ce6c-d8c3-47b0-f02c-15bc15f9bafa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Finished.\n",
            " Tokens: ['The', 'runners', 'were', 'running', 'quickly', 'towards', 'the', 'finishing', 'line']\n",
            " Lemmatized (default): ['The', 'runner', 'were', 'running', 'quickly', 'towards', 'the', 'finishing', 'line']\n",
            " Tagged words: [('The', 'DT'), ('runners', 'NNS'), ('were', 'VBD'), ('running', 'VBG'), ('quickly', 'RB'), ('towards', 'IN'), ('the', 'DT'), ('finishing', 'JJ'), ('line', 'NN')]\n",
            " Lemmatized (with POS): ['The', 'runner', 'be', 'run', 'quickly', 'towards', 'the', 'finishing', 'line']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Stopwords**\n",
        "\n",
        "Stopwords are common words in a language that are usually filtered out before processing text in NLP tasks. These words often don‚Äôt carry significant meaning on their own and are mostly used for grammar or sentence structure.\n",
        "\n"
      ],
      "metadata": {
        "id": "dsCHuL-AmzvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text = \"The cat is sitting on the mat.\"\n",
        "print(text)\n",
        "tokens = word_tokenize(text)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "tokens = [word.lower() for word in tokens]\n",
        "print(\"Lowerwords:\", tokens)\n",
        "filtered = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(\"After Stopword Removal:\", filtered)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ry_79HLv1A7L",
        "outputId": "c2dc7b11-ba36-4f65-bb65-107a65569496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cat is sitting on the mat.\n",
            "Lowerwords: ['the', 'cat', 'is', 'sitting', 'on', 'the', 'mat', '.']\n",
            "After Stopword Removal: ['cat', 'sitting', 'mat', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Named Entity Recognition**\n",
        "\n",
        "Named Entity Recognition (NER) is a Natural Language Processing (NLP) technique used to identify and classify named entities in text into predefined categories such as:\n",
        "\n",
        "### üè∑Ô∏è Named Entity Categories\n",
        "\n",
        "| **Category**     | **Example**             |\n",
        "|------------------|--------------------------|\n",
        "| **Person**       | Elon Musk, Sachin        |\n",
        "| **Organization** | Google, NASA             |\n",
        "| **Location**     | India, New York          |\n",
        "| **Date**         | 26 March 2025            |\n",
        "| **Time**         | 5 PM                     |\n",
        "| **Money**        | $500, ‚Çπ1000              |\n"
      ],
      "metadata": {
        "id": "zb02O2ax2EzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "\n",
        "# Sample text\n",
        "text = \"Elon Musk founded SpaceX in California on 14 March 2002.\"\n",
        "\n",
        "# Tokenize and POS tagging\n",
        "words = nltk.word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "# Named Entity Chunking\n",
        "ne_tree = nltk.ne_chunk(pos_tags)\n",
        "print(\" Named Entities:\")\n",
        "print(ne_tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mjragwo72UpN",
        "outputId": "83699306-85a1-48c2-87fe-02209dc47328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Named Entities:\n",
            "(S\n",
            "  (PERSON Elon/NNP)\n",
            "  (PERSON Musk/NNP)\n",
            "  founded/VBD\n",
            "  (ORGANIZATION SpaceX/NNP)\n",
            "  in/IN\n",
            "  (GPE California/NNP)\n",
            "  on/IN\n",
            "  14/CD\n",
            "  March/NNP\n",
            "  2002/CD\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text pre-processing:**\n",
        "\n",
        "Machines can‚Äôt understand raw text ‚Äî we need to convert words into numerical form (vectors). Here are common techniques:\n",
        "\n",
        "1. One hot encoding\n",
        "2. Bag of words(BOW)\n",
        "3. Term Frequency ‚Äì Inverse Document Frequency (TF-IDF)\n",
        "4. Word2vec\n",
        "5. Avgword2vec"
      ],
      "metadata": {
        "id": "qSyd13VH3g_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **One Hot encoding:**\n",
        "\n",
        "Each word is represented by a binary vector. The length of the vector equals the vocabulary size. Only one position is 1, others are 0."
      ],
      "metadata": {
        "id": "dDVBVt7p4Q8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Install scikit-learn if not already\n",
        "!pip install -q scikit-learn\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#  Sample sentences\n",
        "texts = [\n",
        "    \"apple banana cherry\",\n",
        "    \"banana apple banana\",\n",
        "    \"cherry banana apple\",\n",
        "    \"apple banana apple\"\n",
        "]\n",
        "\n",
        "#  Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer(binary=True)  # binary=True gives One-Hot Encoding\n",
        "\n",
        "# Fit and transform\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Show results\n",
        "print(\" Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"One-Hot Encoded Matrix:\\n\", X.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2piuqGf5LOp",
        "outputId": "963c7327-00e8-4dea-d035-ed41761ccebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Vocabulary: ['apple' 'banana' 'cherry']\n",
            "One-Hot Encoded Matrix:\n",
            " [[1 1 1]\n",
            " [1 1 0]\n",
            " [1 1 1]\n",
            " [1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"apple banana cherry banana apple\"\n",
        "\n",
        "# Step 1: Tokenize sentence into words\n",
        "tokens = sentence.split()\n",
        "\n",
        "# Step 2: Create a vocabulary (unique words)\n",
        "vocab = sorted(set(tokens))\n",
        "print(\" Vocabulary:\", vocab)\n",
        "\n",
        "# Step 3: Create word-to-index mapping\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "print(\" Word to Index Mapping:\", word_to_index)\n",
        "\n",
        "# Step 4: One-Hot Encode each word\n",
        "ohe_vectors = []\n",
        "for word in tokens:\n",
        "    vector = [0] * len(vocab)\n",
        "    vector[word_to_index[word]] = 1\n",
        "    ohe_vectors.append(vector)\n",
        "\n",
        "# Step 5: Print results\n",
        "print(\"\\n One-Hot Encoded Vectors:\")\n",
        "for word, vec in zip(tokens, ohe_vectors):\n",
        "    print(f\"{word}: {vec}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4pCPARE4qYa",
        "outputId": "f1336275-8cc9-4d3f-dd07-5143cd0d485a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Vocabulary: ['apple', 'banana', 'cherry']\n",
            " Word to Index Mapping: {'apple': 0, 'banana': 1, 'cherry': 2}\n",
            "\n",
            " One-Hot Encoded Vectors:\n",
            "apple: [1, 0, 0]\n",
            "banana: [0, 1, 0]\n",
            "cherry: [0, 0, 1]\n",
            "banana: [0, 1, 0]\n",
            "apple: [1, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "\n",
        "Advantage:\n",
        "1. easy to use in python (use scikit-learn )\n",
        "\n",
        "Disadvantage:\n",
        "1. sparse matrix (0,1) --> overfit the ml model\n",
        "2. ML algorithm we need to fix the input size\n",
        "3. No sematic meaning is capture\n",
        "4. Out of vacabulary\n",
        "\n"
      ],
      "metadata": {
        "id": "XOFaOab4435B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **BOW:**\n",
        "Bag of Words is a basic and popular technique to convert text into numerical features for machine learning models.\n",
        "\n",
        " Key Idea:\n",
        " Treat each document/sentence as a ‚Äúbag‚Äù (collection) of words.Ignore grammar & word order (remove stopwords, and lower all words). Just count how many times each word appears."
      ],
      "metadata": {
        "id": "lc79cCaB60n7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import required library\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#  Sample sentences\n",
        "sentences = [\n",
        "    \"apple banana apple\",\n",
        "    \"banana cherry\"\n",
        "]\n",
        "\n",
        "#  Initialize CountVectorizer (BoW)\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the sentences into BoW vectors\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Get feature (vocabulary) names\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "print(\" Vocabulary:\", vocab)\n",
        "\n",
        "# Convert sparse matrix to array for display\n",
        "bow_array = X.toarray()\n",
        "print(\"\\n Bag of Words Vectors:\")\n",
        "for i, vec in enumerate(bow_array):\n",
        "    print(f\"Sentence {i+1}: {vec}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0FTM1xO7JBZ",
        "outputId": "a66a71d9-5283-4994-b297-7e3e9edc12b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Vocabulary: ['apple' 'banana' 'cherry']\n",
            "\n",
            " Bag of Words Vectors:\n",
            "Sentence 1: [2 1 0]\n",
            "Sentence 2: [0 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "Advantage:\n",
        "1. simple and initutive\n",
        "2. fixed size input is used fo ml algorithm\n",
        "\n",
        "Disadvantage:\n",
        "1. Sparse matrix(0,1) -->overfit in ML model\n",
        "2. ordering of word is getting changed\n",
        "3. out of vocabulary\n",
        "4. no semantic meaning is capture"
      ],
      "metadata": {
        "id": "DWfulUEH70T6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **N-grams**\n",
        "\n",
        "An N-gram is a sequence of N words from a given text.N-grams help capture context and word patterns, which are often missed in Bag of Words.Useful for language modeling, text classification, and spam detection.\n",
        "\n",
        "Example: \"New York\" (bigram) is more meaningful than \"New\", \"York\" separately."
      ],
      "metadata": {
        "id": "1-alA3i68VLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample text\n",
        "text = [\"I love natural language processing\"]\n",
        "\n",
        "#  Unigram (n=1)\n",
        "vectorizer_uni = CountVectorizer(ngram_range=(1,1))\n",
        "X_uni = vectorizer_uni.fit_transform(text)\n",
        "print(\" Unigrams:\", vectorizer_uni.get_feature_names_out())\n",
        "\n",
        "#  Bigram (n=2)\n",
        "vectorizer_bi = CountVectorizer(ngram_range=(2,2))\n",
        "X_bi = vectorizer_bi.fit_transform(text)\n",
        "print(\" Bigrams:\", vectorizer_bi.get_feature_names_out())\n",
        "\n",
        "#  Trigram (n=3)\n",
        "vectorizer_tri = CountVectorizer(ngram_range=(3,3))\n",
        "X_tri = vectorizer_tri.fit_transform(text)\n",
        "print(\" Trigrams:\", vectorizer_tri.get_feature_names_out())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeByQ9d59St6",
        "outputId": "07e433ce-b356-48c2-fb19-a5e2ac63cf0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Unigrams: ['language' 'love' 'natural' 'processing']\n",
            " Bigrams: ['language processing' 'love natural' 'natural language']\n",
            " Trigrams: ['love natural language' 'natural language processing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. TF-IDF\n",
        "\n",
        "TF-IDF stands for:\n",
        "\n",
        "1. Term Frequency (TF): How often a word appears in a document.\n",
        "\n",
        "2. Inverse Document Frequency (IDF): How rare or unique a word is across all documents.\n",
        "\n",
        "Goal: Highlight important words in a document and downweight common words like ‚Äúthe‚Äù, ‚Äúis‚Äù, ‚Äúand‚Äù"
      ],
      "metadata": {
        "id": "3nUDq0Qc_lMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#  Sample documents\n",
        "documents = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is amazing\",\n",
        "    \"I enjoy learning new things about AI\"\n",
        "]\n",
        "\n",
        "#  Create TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the documents\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Show the vocabulary\n",
        "print(\" Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# Convert to array for viewing\n",
        "tfidf_matrix = X.toarray()\n",
        "\n",
        "# Print TF-IDF matrix\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(tfidf_matrix, columns=vectorizer.get_feature_names_out())\n",
        "print(\"\\n TF-IDF Matrix:\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zFJoRa4_9Vg",
        "outputId": "a0bb32ca-9bb9-420a-dd9f-451b7f487511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Vocabulary: ['about' 'ai' 'amazing' 'enjoy' 'is' 'learning' 'love' 'machine' 'new'\n",
            " 'things']\n",
            "\n",
            " TF-IDF Matrix:\n",
            "      about        ai   amazing     enjoy  ...      love   machine       new    things\n",
            "0  0.000000  0.000000  0.000000  0.000000  ...  0.720333  0.547832  0.000000  0.000000\n",
            "1  0.000000  0.000000  0.584483  0.000000  ...  0.000000  0.444514  0.000000  0.000000\n",
            "2  0.432385  0.432385  0.000000  0.432385  ...  0.000000  0.000000  0.432385  0.432385\n",
            "\n",
            "[3 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "\n",
        "Advantage:\n",
        "\n",
        "1. easy and initiative\n",
        "2. fixed vocabulary size\n",
        "3. word importance is getting capture\n",
        "\n",
        "Disadvantage\n",
        "\n",
        "1. Saprse matrix\n",
        "2. OOV"
      ],
      "metadata": {
        "id": "7ojoVjtmBf2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Word2Vec**\n",
        "\n",
        "Word2Vec is a popular technique in NLP to convert words into vectors (dense numerical representations) that capture meaning and context.\n",
        "\n",
        "Unlike Bag of Words or TF-IDF, Word2Vec doesn‚Äôt just count ‚Äî it learns from context using a neural network.\n",
        "\n",
        "###  Word2Vec Models\n",
        "\n",
        "| **Model**                      | **Goal**                                             |\n",
        "|-------------------------------|------------------------------------------------------|\n",
        "| **CBOW (Continuous Bag of Words)** | Predict the current word based on surrounding context |\n",
        "| **Skip-gram**                 | Predict the surrounding context words from the current word |\n"
      ],
      "metadata": {
        "id": "sJ7nD2AYCfbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Force reinstall compatible numpy version\n",
        "!pip install numpy==1.23.5 --upgrade --force-reinstall\n",
        "\n",
        "# Step 2: Reinstall gensim to recompile properly with the new numpy\n",
        "!pip install gensim --upgrade --force-reinstall\n"
      ],
      "metadata": {
        "id": "enlTt39KEfci",
        "outputId": "98551dee-0ec3-44c7-b4dc-604e0742d726",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gensim if not already\n",
        "!pip install -q gensim\n",
        "\n",
        "#  Download Google‚Äôs pretrained Word2Vec model (if not already present)\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Load the model (this will download it if not cached)\n",
        "print(\" Loading Google Word2Vec Model...\")\n",
        "model = api.load(\"word2vec-google-news-300\")\n",
        "print(\" Model loaded!\")\n",
        "\n",
        "#  Test the model\n",
        "# 1. Get the vector for a word\n",
        "word = \"king\"\n",
        "print(f\"\\n Vector for '{word}':\")\n",
        "print(model[word])  # 300-dimensional vector\n",
        "\n",
        "# 2. Find similar words\n",
        "print(f\"\\n Words similar to '{word}':\")\n",
        "print(model.most_similar(word))\n",
        "\n",
        "# 3. Word analogy example\n",
        "print(\"\\n Word analogy: 'king' - 'man' + 'woman' ‚âà\")\n",
        "print(model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub3e-MQrBZt6",
        "outputId": "8c85b054-b257-458d-9def-1a3f7695c188"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loading Google Word2Vec Model...\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            " Model loaded!\n",
            "\n",
            " Vector for 'king':\n",
            "[ 1.25976562e-01  2.97851562e-02  8.60595703e-03  1.39648438e-01\n",
            " -2.56347656e-02 -3.61328125e-02  1.11816406e-01 -1.98242188e-01\n",
            "  5.12695312e-02  3.63281250e-01 -2.42187500e-01 -3.02734375e-01\n",
            " -1.77734375e-01 -2.49023438e-02 -1.67968750e-01 -1.69921875e-01\n",
            "  3.46679688e-02  5.21850586e-03  4.63867188e-02  1.28906250e-01\n",
            "  1.36718750e-01  1.12792969e-01  5.95703125e-02  1.36718750e-01\n",
            "  1.01074219e-01 -1.76757812e-01 -2.51953125e-01  5.98144531e-02\n",
            "  3.41796875e-01 -3.11279297e-02  1.04492188e-01  6.17675781e-02\n",
            "  1.24511719e-01  4.00390625e-01 -3.22265625e-01  8.39843750e-02\n",
            "  3.90625000e-02  5.85937500e-03  7.03125000e-02  1.72851562e-01\n",
            "  1.38671875e-01 -2.31445312e-01  2.83203125e-01  1.42578125e-01\n",
            "  3.41796875e-01 -2.39257812e-02 -1.09863281e-01  3.32031250e-02\n",
            " -5.46875000e-02  1.53198242e-02 -1.62109375e-01  1.58203125e-01\n",
            " -2.59765625e-01  2.01416016e-02 -1.63085938e-01  1.35803223e-03\n",
            " -1.44531250e-01 -5.68847656e-02  4.29687500e-02 -2.46582031e-02\n",
            "  1.85546875e-01  4.47265625e-01  9.58251953e-03  1.31835938e-01\n",
            "  9.86328125e-02 -1.85546875e-01 -1.00097656e-01 -1.33789062e-01\n",
            " -1.25000000e-01  2.83203125e-01  1.23046875e-01  5.32226562e-02\n",
            " -1.77734375e-01  8.59375000e-02 -2.18505859e-02  2.05078125e-02\n",
            " -1.39648438e-01  2.51464844e-02  1.38671875e-01 -1.05468750e-01\n",
            "  1.38671875e-01  8.88671875e-02 -7.51953125e-02 -2.13623047e-02\n",
            "  1.72851562e-01  4.63867188e-02 -2.65625000e-01  8.91113281e-03\n",
            "  1.49414062e-01  3.78417969e-02  2.38281250e-01 -1.24511719e-01\n",
            " -2.17773438e-01 -1.81640625e-01  2.97851562e-02  5.71289062e-02\n",
            " -2.89306641e-02  1.24511719e-02  9.66796875e-02 -2.31445312e-01\n",
            "  5.81054688e-02  6.68945312e-02  7.08007812e-02 -3.08593750e-01\n",
            " -2.14843750e-01  1.45507812e-01 -4.27734375e-01 -9.39941406e-03\n",
            "  1.54296875e-01 -7.66601562e-02  2.89062500e-01  2.77343750e-01\n",
            " -4.86373901e-04 -1.36718750e-01  3.24218750e-01 -2.46093750e-01\n",
            " -3.03649902e-03 -2.11914062e-01  1.25000000e-01  2.69531250e-01\n",
            "  2.04101562e-01  8.25195312e-02 -2.01171875e-01 -1.60156250e-01\n",
            " -3.78417969e-02 -1.20117188e-01  1.15234375e-01 -4.10156250e-02\n",
            " -3.95507812e-02 -8.98437500e-02  6.34765625e-03  2.03125000e-01\n",
            "  1.86523438e-01  2.73437500e-01  6.29882812e-02  1.41601562e-01\n",
            " -9.81445312e-02  1.38671875e-01  1.82617188e-01  1.73828125e-01\n",
            "  1.73828125e-01 -2.37304688e-01  1.78710938e-01  6.34765625e-02\n",
            "  2.36328125e-01 -2.08984375e-01  8.74023438e-02 -1.66015625e-01\n",
            " -7.91015625e-02  2.43164062e-01 -8.88671875e-02  1.26953125e-01\n",
            " -2.16796875e-01 -1.73828125e-01 -3.59375000e-01 -8.25195312e-02\n",
            " -6.49414062e-02  5.07812500e-02  1.35742188e-01 -7.47070312e-02\n",
            " -1.64062500e-01  1.15356445e-02  4.45312500e-01 -2.15820312e-01\n",
            " -1.11328125e-01 -1.92382812e-01  1.70898438e-01 -1.25000000e-01\n",
            "  2.65502930e-03  1.92382812e-01 -1.74804688e-01  1.39648438e-01\n",
            "  2.92968750e-01  1.13281250e-01  5.95703125e-02 -6.39648438e-02\n",
            "  9.96093750e-02 -2.72216797e-02  1.96533203e-02  4.27246094e-02\n",
            " -2.46093750e-01  6.39648438e-02 -2.25585938e-01 -1.68945312e-01\n",
            "  2.89916992e-03  8.20312500e-02  3.41796875e-01  4.32128906e-02\n",
            "  1.32812500e-01  1.42578125e-01  7.61718750e-02  5.98144531e-02\n",
            " -1.19140625e-01  2.74658203e-03 -6.29882812e-02 -2.72216797e-02\n",
            " -4.82177734e-03 -8.20312500e-02 -2.49023438e-02 -4.00390625e-01\n",
            " -1.06933594e-01  4.24804688e-02  7.76367188e-02 -1.16699219e-01\n",
            "  7.37304688e-02 -9.22851562e-02  1.07910156e-01  1.58203125e-01\n",
            "  4.24804688e-02  1.26953125e-01  3.61328125e-02  2.67578125e-01\n",
            " -1.01074219e-01 -3.02734375e-01 -5.76171875e-02  5.05371094e-02\n",
            "  5.26428223e-04 -2.07031250e-01 -1.38671875e-01 -8.97216797e-03\n",
            " -2.78320312e-02 -1.41601562e-01  2.07031250e-01 -1.58203125e-01\n",
            "  1.27929688e-01  1.49414062e-01 -2.24609375e-02 -8.44726562e-02\n",
            "  1.22558594e-01  2.15820312e-01 -2.13867188e-01 -3.12500000e-01\n",
            " -3.73046875e-01  4.08935547e-03  1.07421875e-01  1.06933594e-01\n",
            "  7.32421875e-02  8.97216797e-03 -3.88183594e-02 -1.29882812e-01\n",
            "  1.49414062e-01 -2.14843750e-01 -1.83868408e-03  9.91210938e-02\n",
            "  1.57226562e-01 -1.14257812e-01 -2.05078125e-01  9.91210938e-02\n",
            "  3.69140625e-01 -1.97265625e-01  3.54003906e-02  1.09375000e-01\n",
            "  1.31835938e-01  1.66992188e-01  2.35351562e-01  1.04980469e-01\n",
            " -4.96093750e-01 -1.64062500e-01 -1.56250000e-01 -5.22460938e-02\n",
            "  1.03027344e-01  2.43164062e-01 -1.88476562e-01  5.07812500e-02\n",
            " -9.37500000e-02 -6.68945312e-02  2.27050781e-02  7.61718750e-02\n",
            "  2.89062500e-01  3.10546875e-01 -5.37109375e-02  2.28515625e-01\n",
            "  2.51464844e-02  6.78710938e-02 -1.21093750e-01 -2.15820312e-01\n",
            " -2.73437500e-01 -3.07617188e-02 -3.37890625e-01  1.53320312e-01\n",
            "  2.33398438e-01 -2.08007812e-01  3.73046875e-01  8.20312500e-02\n",
            "  2.51953125e-01 -7.61718750e-02 -4.66308594e-02 -2.23388672e-02\n",
            "  2.99072266e-02 -5.93261719e-02 -4.66918945e-03 -2.44140625e-01\n",
            " -2.09960938e-01 -2.87109375e-01 -4.54101562e-02 -1.77734375e-01\n",
            " -2.79296875e-01 -8.59375000e-02  9.13085938e-02  2.51953125e-01]\n",
            "\n",
            " Words similar to 'king':\n",
            "[('kings', 0.7138045430183411), ('queen', 0.6510956883430481), ('monarch', 0.6413194537162781), ('crown_prince', 0.6204220056533813), ('prince', 0.6159993410110474), ('sultan', 0.5864824056625366), ('ruler', 0.5797567367553711), ('princes', 0.5646552443504333), ('Prince_Paras', 0.5432944297790527), ('throne', 0.5422105193138123)]\n",
            "\n",
            " Word analogy: 'king' - 'man' + 'woman' ‚âà\n",
            "[('queen', 0.7118193507194519)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\" Vector for 'king':\", model['king'][:10])  # print first 10 values\n",
        "\n",
        "print(\" Similar to 'king':\", model.most_similar('king')[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaWUdrBqHWVN",
        "outputId": "6fae6dc1-0919-410a-f0dc-fbf43228acf6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Vector for 'king': [ 0.12597656  0.02978516  0.00860596  0.13964844 -0.02563477 -0.03613281\n",
            "  0.11181641 -0.19824219  0.05126953  0.36328125]\n",
            " Similar to 'king': [('kings', 0.7138045430183411), ('queen', 0.6510956883430481), ('monarch', 0.6413194537162781)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "\n",
        "Advantage:\n",
        "\n",
        "1. sparse matrix\n",
        "2. sematic information is getting captured\n",
        "3. vocabulary size is fixed\n",
        "4. oov is also solved here.\n",
        "\n"
      ],
      "metadata": {
        "id": "AnbiRZubHfFm"
      }
    }
  ]
}