# NLP_basics

##  Natural Language Processing (NLP) - Basics

This project covers fundamental NLP preprocessing techniques to prepare and convert raw text into meaningful numerical representations for machine learning and deep learning tasks.

###  Core Concepts Covered:

- **Tokenization**: Breaking text into words or sentences.
- **Lowercasing**: Converting all text to lowercase for uniformity.
- **Stopword Removal**: Eliminating common, low-information words like "the", "is", "and".
- **Stemming**: Reducing words to their root form (e.g., "running" → "run").
- **Lemmatization**: Mapping words to their dictionary form with context (e.g., "better" → "good").
- **N-Grams**: Creating word combinations (unigrams, bigrams, trigrams) to capture context.
- **Bag of Words (BoW)**: Representing text as word frequency vectors.
- **TF-IDF**: Weighing words based on importance across documents.
- **Word2Vec**: Generating dense word embeddings that capture semantic meaning using CBOW and Skip-gram models.
- **Named Entity Recognition (NER)**: Identifying entities like people, places, and organizations in text.

###  Example Tools Used:
- NLTK
- spaCy
- TextBlob
- scikit-learn
- gensim (for Word2Vec)

###  Goal:
To understand and implement foundational NLP techniques used in text classification, search engines, chatbots, and language models.

